# ğŸ¯ What is Reinforcement Learning (RL)?

## ğŸ§  What is Reinforcement Learning?

> **Official Definition**:  
> **Reinforcement Learning (RL)** is a type of Machine Learning where an **agent** learns to make **sequential decisions** by **performing actions** in an **environment** to **maximize cumulative rewards** over time.

---

<div style="text-align: center;">
    <img src="images/reinforcement-learning.gif" alt="reinforcement-learning" style="border-radius: 20px; width: 30%;">
</div>

---

ğŸ“Œ **In simpler words**:  
RL is like **training a puppy ğŸ¶**:

- If it sits â†’ give it a treat ğŸ– (reward).
- If it chews your shoe â†’ scold it âŒ (punishment).

Over time, the puppy **learns good behavior** by trying, failing, and adjusting â€” just like an RL agent!

---

## ğŸŒ Key Concepts in Reinforcement Learning

| Concept        | Meaning                                                          |
| :------------- | :--------------------------------------------------------------- |
| ğŸ§‘â€ğŸ“ Agent    | The learner/decision maker (like the robot or AI player)         |
| ğŸŒ Environment | Where the agent operates and makes decisions (game, maze, world) |
| ğŸ¯ Action      | A choice made by the agent (move left, pick up object)           |
| ğŸ“‹ State       | The current situation/context of the environment                 |
| ğŸ† Reward      | Feedback signal (good or bad) based on the action                |
| ğŸ—ºï¸ Policy      | The strategy the agent follows to pick actions                   |

ğŸ“Œ **Easy Tip**:

- **Agent** acts based on the **State**.
- Gets a **Reward**.
- Updates its **Policy** to act better next time!

---

## ğŸ—ï¸ How Does Reinforcement Learning Work?

ğŸ“Œ The full loop:

<div style="text-align: center;">

```mermaid
flowchart TD
    State[ğŸ“‹ Observe State] --> Action["ğŸ¯ Choose Action (Based on Policy)"]
    Action --> Environment[ğŸŒ Environment Responds]
    Environment --> NewState[ğŸ“‹ New State + ğŸ† Reward]
    NewState --> UpdatePolicy["ğŸ”„ Update Policy (Learn from reward)"]
    UpdatePolicy --> State
```

</div>

ğŸ“Œ **Step-by-step**:

1. Agent **observes** the current **State**.
2. Agent **selects an Action** according to its **Policy**.
3. **Environment responds** with a **Reward** and a new **State**.
4. Agent **updates its Policy** based on the Reward.
5. Repeat â¡ï¸ Agent gets smarter over time!

ğŸ“Œ **Goal**:  
**Maximize total reward** over the long term, not just immediate rewards.

---

## ğŸƒâ€â™‚ï¸ Real-World Example: Robot Navigating a Maze

<div style="text-align: center;">
  <img src="images/robot-maze.png" alt="Robot Navigating a Maze" style="border-radius: 10px; width: 40%;">
</div>

---

ğŸ“Œ Scenario:

- Agent = Robot ğŸ¤–
- Environment = Maze ğŸ§©
- Action = Move (up, down, left, right)
- State = Current position in maze
- Reward =
  - -1 for each move (to encourage quick solutions),
  - -10 for hitting a wall ğŸš§,
  - +100 for reaching the exit ğŸ¯.

ğŸ“Œ Over time:

- The robot **learns** to reach the exit quickly
- **Avoids walls** to **maximize cumulative rewards**.

---

## ğŸ® Applications of Reinforcement Learning

| Field                  | Example                                     |
| :--------------------- | :------------------------------------------ |
| ğŸ•¹ï¸ Gaming              | Training AI to play Chess â™Ÿï¸, Go, Dota 2 ğŸ® |
| ğŸ¤– Robotics            | Navigation, object manipulation             |
| ğŸ’µ Finance             | Optimizing stock trading strategies ğŸ“ˆ      |
| ğŸ¥ Healthcare          | Optimizing personalized treatment plans ğŸ§¬  |
| ğŸš— Autonomous Vehicles | Decision-making for safe driving ğŸ›£ï¸         |

---

## ğŸ§  What is RLHF (Reinforcement Learning from Human Feedback)?

> **Official Definition**:  
> **RLHF** uses **human feedback** to help train RL agents or models, ensuring they **align better** with **human goals, values, and preferences**.

ğŸ“Œ **Simple way to think about it**:

- Normally, RL agents are trained with a **predefined reward function**.
- In RLHF, **humans help define rewards** by judging outputs.

ğŸ“Œ **Real-world Example**:

- Fine-tuning ChatGPT ğŸ§ :
  - Human reviewers grade model responses.
  - AI learns to **prefer human-preferred answers** over technical but robotic ones.

---

## ğŸ—ï¸ How RLHF Works (Process)

<div style="text-align: center;">
    <img src="images/RLHF-process.png" alt="RLHF Process" style="border-radius: 10px; width: 60%;">
</div>

---

```mermaid
flowchart LR
    HumanPrompts[ğŸ“ Human-Generated Prompts + Responses] --> FineTune[ğŸ¯ Fine-tune Base Model]
    FineTune --> GenerateResponses[ğŸ¤– Model Generates New Responses]
    GenerateResponses --> HumanJudges[ğŸ§‘â€âš–ï¸ Humans Score Responses]
    HumanJudges --> RewardModel[ğŸ† Build Reward Model]
    RewardModel --> RLTraining[ğŸ”„ Reinforce Model using Reward Signals]
```

---

ğŸ“Œ **Step-by-Step**:

1. **Collect human responses** to prompts.
2. **Fine-tune a model** on this dataset.
3. Model **generates its own answers**.
4. Humans **rate** which answers are better.
5. Build a **Reward Model** that mimics human judgments.
6. **Retrain the model** using this reward feedback.

---

## ğŸ§© Real-world Example: Internal Knowledge Chatbot

ğŸ“Œ Scenario:

- Build a chatbot for employees to answer company questions.

ğŸ“Œ Steps:

- Human-written Q&A examples (e.g., "Where is HR located?")
- Fine-tune the chatbot on this data.
- Let the chatbot generate answers.
- Humans rate which answers they prefer.
- Train a reward model.
- Optimize chatbot to maximize human preference â¡ï¸ smarter, human-like answers!

---

## ğŸ¯ Mini Recap: Key Points

ğŸ“Œ **Reinforcement Learning**:  
Agent learns by acting, getting rewards, adjusting actions over time.

ğŸ“Œ **RLHF**:  
Enhance RL with **human feedback** to make the model behave more **aligned with human expectations**.

ğŸ“Œ **Core of GenAI today**:  
LLMs (like ChatGPT) use RLHF to sound natural and helpful!

---

## âœï¸ Smart Cheatsheet

| Term        | Meaning                                       |
| :---------- | :-------------------------------------------- |
| RL          | Learn by trying, receiving rewards            |
| Agent       | Decision maker (robot, AI, chatbot)           |
| Environment | Where the agent acts (maze, game, real world) |
| Reward      | Positive or negative feedback                 |
| Policy      | Strategy to pick next action                  |
| RLHF        | Humans guide the rewards for better alignment |

## Extra knowledge

- [Example of RH â€AI Learns to Escapeâ€](https://youtu.be/2tamH76Tjvw)
- [AI Warehouse](https://www.youtube.com/@aiwarehouse)
- [Reinforcement Learning](https://www.kaggle.com/learn/machine-learning/reinforcement-learning)
- [RLHF](https://www.kaggle.com/learn/machine-learning/rlhf)
