# ğŸ”  Tokenization in Generative AI: Full and Clear Guide

## ğŸ§  **What is Tokenization?**

> **Tokenization** = Converting raw human text into smaller pieces called **tokens** that a model can understand and process.

---

<div style="text-align: center;">
    <img src="images/tokenizer.png" alt="tokenizer" style="border-radius: 20px; width: 60%;">
</div>

---

ğŸ“Œ In simple words:

- **Text â” Tokens â” Model processes tokens.**
- AI models don't "read" like humans â€” they read **tokens**!

---

## ğŸ“¦ Two Main Types of Tokenization

| Type             | Description                                        | Example                                 |
| :--------------- | :------------------------------------------------- | :-------------------------------------- |
| ğŸ“ Word-Based    | Split text into full words.                        | `"Hello world"` â” `["Hello", "world"]`  |
| ğŸ”¤ Subword-Based | Split inside words if necessary (smart splitting). | `"unhappiness"` â” `["un", "happiness"]` |

---

## ğŸ› ï¸ **How Tokenization Actually Works**

### **1ï¸âƒ£ Encoding (Text â” Tokens)**

- When you **input text**, the tokenizer **splits** it according to its vocabulary rules.
- Every **token gets assigned a unique integer ID**.

---

<div style="text-align: center;">
    <img src="images/tokenization-encoding.png" alt="tokenization-encoding" style="border-radius: 10px; width: 60%;">
</div>

---

Example:

| Text          | Tokens             | IDs           |
| :------------ | :----------------- | :------------ |
| "Hello world" | ["Hello", "world"] | [15496, 2159] |

âœ… Models **don't see text**, only the list of token IDs!

---

### **2ï¸âƒ£ Decoding (Tokens â” Text)**

- When the model **outputs predictions**, it produces **IDs**.
- The tokenizer **decodes** these IDs back into readable human text.

---

<div style="text-align: center;">
    <img src="images/tokenization-decoding.png" alt="tokenization-decoding" style="border-radius: 10px; width: 60%;">
</div>

---

Example:

| IDs           | Tokens             | Text          |
| :------------ | :----------------- | :------------ |
| [15496, 2159] | ["Hello", "world"] | "Hello world" |

âœ… Decoding = **reconstructing the original (or very close) sentence.**

---

## **ğŸ” How/When Subword Tokenization Works**

- If a word **isn't fully in the vocabulary**, the tokenizer **splits it into smaller parts (subwords)**.
- These subwords are then tokenized separately.

---

<div style="text-align: center;">
    <img src="images/subword-tokenization.png"
         alt="subword-tokenization"
         style="border-radius: 10px; width: 60%;">
</div>

---

Example:

| Word            | Subword Tokens        |
| :-------------- | :-------------------- |
| "unhappiness"   | ["un", "happiness"]   |
| "autogenerated" | ["auto", "generated"] |

### ğŸš¨ **When Subword Tokenization Happens?**

| Scenario                      | What Happens                                      |
| :---------------------------- | :------------------------------------------------ |
| Word **exists** in vocabulary | Directly map to one token.                        |
| Word **not found** as whole   | Break into subwords to find parts.                |
| Completely unknown parts      | Use fallback subwords or unknown token (`<unk>`). |

âœ… This is **super important** for **handling typos**, **rare words**, **technical terms**, and **different languages**!

---

## ğŸ¯ **Practical Tip**

> You can **experiment with tokenization** and **see subtokens** live at:  
> [ğŸ‘‰ OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer)

âœ… Try writing weird or long words â€” you'll notice when the model breaks them into subwords!

---

## ğŸ§  **Quick Summary**

| Concept              | What It Means                        |
| :------------------- | :----------------------------------- |
| Tokenization         | Break text into units (tokens).      |
| Encoding             | Text â” Token IDs.                    |
| Decoding             | Token IDs â” Text.                    |
| Subword Tokenization | Splitting rare/unseen words smartly. |

---

ğŸ“Œ **Smart Memory Tip**:

> "**Tokenization = Splitting and Rebuilding Language for AI.**" ğŸ› ï¸  
> "**Subword Tokenization = AIâ€™s backup plan when words get complicated!**" ğŸ”¥
