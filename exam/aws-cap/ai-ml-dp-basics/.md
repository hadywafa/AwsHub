# ğŸ“š Model Evaluation

## ğŸ§  **What is Model Evaluation?**

> **Definition**:  
> **Model Evaluation** measures **how good** or **bad** a machine learning model is at solving the intended problem,  
> using the right metrics depending on whether the task is **Classification** or **Regression**.

ğŸ‘‰ **Simply**:

- For predicting **categories** â†’ use **classification metrics** ğŸ·ï¸.
- For predicting **numbers** â†’ use **regression metrics** ğŸ“ˆ.

---

## ğŸ›ï¸ **Evaluation Branches**

<div align="center">

```mermaid
flowchart TD
    Evaluation[ğŸ§  Model Evaluation]
    Evaluation --> Classification[ğŸ·ï¸ Classification Problems]
    Evaluation --> Regression[ğŸ“ˆ Regression Problems]

    Classification --> ConfusionMatrix[ğŸ§© Confusion Matrix]
    Classification --> PrecisionRecallAUC[ğŸ¯ Precision, Recall, AUC]

    Regression --> MSE_RMSE_MAE_MAPE[ğŸ“Š MSE, RMSE, MAE, MAPE]
    Regression --> RSquared["ğŸ“ˆ RÂ² (R-Squared)"]
```

</div>

---

## ğŸ·ï¸ **Classification Evaluation**

> **Classification** = Predict **which category** the data belongs to.

---

<div align="center">
    <img src="images/classification-evaluation.png" alt="Classification Evaluation" style="width: 60%; border-radius: 20px;"/>
</div>

---

âœ… **Examples**:

- Spam vs Not Spam ğŸ“§.
- Dog vs Cat ğŸ¶ğŸ±.

---

### ğŸ§© 1. **Confusion Matrix**

--

âœ… A table showing **correct vs incorrect predictions**:

| Actual \ Predicted | Positive               | Negative               |
| :----------------- | :--------------------- | :--------------------- |
| Positive           | True Positive (TP) âœ…  | False Negative (FN) âŒ |
| Negative           | False Positive (FP) âŒ | True Negative (TN) âœ…  |

âœ… **Meaning**:

- **TP (True Positive)**: Predicted positive correctly.
- **TN (True Negative)**: Predicted negative correctly.
- **FP (False Positive)**: Wrongly predicted positive.
- **FN (False Negative)**: Missed predicting a positive case.

---

### ğŸ¯ 2. **Precision, Recall, F1 Score, AUC**

âœ… **Metrics and Formulas**:

| Metric                     | Formula                                                                                                     | Best Used When                                   |
| :------------------------- | :---------------------------------------------------------------------------------------------------------- | :----------------------------------------------- |
| Precision                  | $\text{Precision} = \frac{TP}{TP + FP}$                                                                     | Costly false positives (e.g., fraud detection)   |
| Recall (Sensitivity)       | $\text{Recall} = \frac{TP}{TP + FN}$                                                                        | Costly false negatives (e.g., medical diagnosis) |
| F1 Score                   | $\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ | Need balance between precision and recall        |
| AUC (Area Under ROC Curve) | Computed from ROC Curve                                                                                     | Overall model discrimination ability             |

âœ… **Quick Hints**:

- **Precision**: How many predicted positives are correct.
- **Recall**: How many actual positives were caught.
- **F1 Score**: Good if classes are imbalanced.
- **AUC**: Good for comparing models across thresholds.

âœ… **Simple Comparison**:

| Metric    | Focus                                 |
| :-------- | :------------------------------------ |
| Precision | Accuracy of positives predicted       |
| Recall    | Ability to find positives             |
| F1 Score  | Tradeoff between Precision and Recall |
| AUC       | Overall separation between classes    |

---

## ğŸ“ˆ **Regression Evaluation**

> **Regression** = Predict a **continuous value** (number).

---

<div align="center">
    <img src="images/regression-evaluation.png" alt="Regression Evaluation" style="width: 60%; border-radius: 20px;"/>
</div>

---

âœ… **Examples**:

- Predicting house prices ğŸ .
- Predicting temperatures ğŸŒ¡ï¸.

---

### ğŸ“Š 1. **MSE, RMSE, MAE, MAPE**

âœ… **Metrics and Formulas**:

| Metric                                | Formula                                                                                       | Interpretation                |
| :------------------------------------ | :-------------------------------------------------------------------------------------------- | :---------------------------- |
| MAE (Mean Absolute Error)             | $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} y_i - \hat{y}_i $$                                 | Average absolute error        |
| MAPE (Mean Absolute Percentage Error) | $$ \text{MAPE} = \frac{100}{n} \sum_{i=1}^{n} \left\| \frac{y_i - \hat{y}_i}{y_i} \right\| $$ | Average percentage error      |
| MSE (Mean Squared Error)              | $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$                             | Average squared error         |
| RMSE (Root Mean Squared Error)        | $$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$                     | Root of average squared error |

âœ… **Quick Hints**:

- **MAE** â†’ Direct, easy-to-interpret average error (unit = same as output).
- **MAPE** â†’ Tells you percentage error â€” very useful for comparing across different scales.
- **MSE / RMSE** â†’ Punishes larger errors more (important if big mistakes are very costly).

âœ… **Simple Comparison**:

| Metric | Focus                                 |
| :----- | :------------------------------------ |
| MAE    | Average simple mistake                |
| MAPE   | How wrong you are in percentage terms |
| MSE    | Emphasizes bigger mistakes            |
| RMSE   | Bigger mistakes, same unit as output  |

---

### ğŸ“ˆ 2. **RÂ² (R-Squared)**

âœ… **What It Measures**:

- How much of the **variance** in the target variable is **explained** by the model.

âœ… **Formula**:

$$
R^2 = 1 - \frac{\sum (y_i - \hat{y}\_i)^2}{\sum (y_i - \bar{y})^2}
$$

Where:

- ( $ \hat{y}\_i $ ) = predicted value
- ( $y_i $) = actual value
- ( $ \bar{y} $ ) = mean of actual values

âœ… **Ranges**:

| RÂ² Value | Meaning                       |
| :------- | :---------------------------- |
| 1.0      | Perfect prediction âœ…         |
| 0.0      | No predictive power âŒ        |
| Negative | Worse than random guessing ğŸ˜± |

âœ… **Quick Example**:

- RÂ² = 0.85 â” 85% of variations in house prices explained by model features.

âœ… **Quick Hint**:

- High RÂ² is good, but **be careful**:  
  Sometimes a high RÂ² can come from **overfitting** if too many irrelevant features are added!

---

## âœï¸ **Mini Smart Recap**

| Branch            | Key Metrics                                  | Meaning                                  |
| :---------------- | :------------------------------------------- | :--------------------------------------- |
| ğŸ·ï¸ Classification | Confusion Matrix, Precision, Recall, F1, AUC | Catching categories correctly            |
| ğŸ“ˆ Regression     | MAE, MSE, RMSE, MAPE, RÂ²                     | Predicting continuous numbers accurately |

âœ… **Simple Tip**:

- Predicting a label? â¡ï¸ Classification metrics ğŸ·ï¸.
- Predicting a number? â¡ï¸ Regression metrics ğŸ“ˆ.

âœ… **Choosing Between Metrics**:

| Problem Focus          | Best Metric |
| :--------------------- | :---------- |
| Costly False Positives | Precision   |
| Costly False Negatives | Recall      |
| Need Balance           | F1 Score    |
| Big Errors Hurt        | RMSE        |
| Easy Interpretation    | MAE, MAPE   |
| Variance Explained     | RÂ²          |
