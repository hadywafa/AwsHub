# â˜€ï¸ **Amazon Aurora Architecture**

Amazon Aurora is a **cloud-native relational database** designed to provide **high availability, scalability, and performance**. Unlike traditional relational database systems, **`Aurora separates compute from storage`**, making it **more resilient, scalable, and cost-efficient**.

---

<div style="text-align: center">
    <img src="images/aurora-architecture.png" alt="Aurora Architecture Diagram" style="width: 60%;border-radius: 10px">
</div>

---

## ğŸ“Œ **Core Architectural Components of Aurora**

Amazon Aurora consists of **two main layers** that work together to provide its advanced capabilities:

### **ğŸ”¹ 1. Compute Layer (Database Instances)**

- The **compute layer** runs the database engine (**Aurora MySQL or Aurora PostgreSQL**).
- It processes **SQL queries, transactions, and indexing** but **does not store data locally**.
- The **compute instances connect to a shared distributed storage layer** for data retrieval.
- Supports:
  - **Primary instance (Writer)** â€“ Handles **both read and write operations**.
  - **Up to 15 Read Replicas per Region** â€“ For **scaling read workloads** and **automatic failover**.
- **Failover Process**:
  - If the **writer fails**, **Aurora promotes a read replica** to a new primary **`within ~30 seconds`**.

### **ğŸ”¹ 2. Storage Layer (Auroraâ€™s Distributed Storage)**

- **Fully managed, auto-scaling distributed storage** spanning **3 AWS Availability Zones (AZs)**.
- **Automatically scales from 10GB to 128TiB** without downtime or manual provisioning.
- **Data is stored in 10GB "Protection Groups"**, each replicated **six times (two per AZ)** for fault tolerance.
- **Quorum-based replication**: A write is **confirmed when at least 4 out of 6 storage nodes acknowledge it**.
- **Read replicas share the same storage**, reducing replication lag to **`<100ms`**.

ğŸ“Œ **How This Differs from Traditional RDS**

- In **regular RDS**, each database instance has its own **dedicated storage volume**.
- In **Aurora**, all instances **share the same distributed storage layer**, eliminating replication delays.

---

## ğŸ”„ **How Aurora Handles Reads & Writes**

### **âœï¸ Aurora Write Process**

âœ” **Writes only happen in the primary instance (Writer).**  
âœ” **Aurora does NOT write entire data pages** to storage â€“ it **stores only redo log records**.  
âœ” **No WAL (Write-Ahead Log) shipping** â€“ Instead, writes are sent **directly to Aurora storage nodes**.  
âœ” A write is **considered committed when 4 out of 6 storage nodes acknowledge it**.

```mermaid
sequenceDiagram
    participant Client as Client
    participant Primary as Aurora Primary DB (Writer)
    participant Storage1 as Storage Node (AZ1)
    participant Storage2 as Storage Node (AZ1)
    participant Storage3 as Storage Node (AZ2)
    participant Storage4 as Storage Node (AZ2)
    participant Storage5 as Storage Node (AZ3)
    participant Storage6 as Storage Node (AZ3)

    Client->>Primary: Write Request (INSERT/UPDATE)
    Primary->>Storage1: Send Redo Log
    Primary->>Storage2: Send Redo Log
    Primary->>Storage3: Send Redo Log
    Primary->>Storage4: Send Redo Log
    Primary->>Storage5: Send Redo Log
    Primary->>Storage6: Send Redo Log

    Storage1-->>Primary: ACK (Confirmed)
    Storage2-->>Primary: ACK (Confirmed)
    Storage3-->>Primary: ACK (Confirmed)
    Storage4-->>Primary: ACK (Confirmed)
    Storage5--X Primary: Timeout
    Storage6--X Primary: Timeout

    Primary->>Client: Write Committed (4/6 Quorum Achieved)
```

ğŸ“Œ **Key Takeaways**

- Aurora **does not modify database pages immediately**â€”it first **stores redo logs in storage nodes**.
- **Less I/O overhead**, increasing performance significantly.
- Even if **1 or 2 storage nodes fail**, writes continue **without data loss**.

---

### **ğŸ“– Aurora Read Process**

âœ” Read replicas **do not maintain separate database copies**.  
âœ” Instead, all read replicas **query the shared storage layer directly**.  
âœ” This **eliminates replication lag** and **allows instant failover**.

```mermaid
sequenceDiagram
    participant Client as Client
    participant Replica as Aurora Read Replica
    participant Storage as Aurora Shared Storage (Distributed Data)

    Client->>Replica: Read Request (SELECT)
    Replica->>Storage: Fetch Latest Committed Data
    Storage-->>Replica: Return Data (Low-Latency)
    Replica-->>Client: Query Result
```

ğŸ“Œ **Key Takeaways:**

- **No need for WAL log replay**â€”replicas instantly access the same data.
- **Near-zero replication lag** (**<100ms**).
- Up to **15 read replicas** for **high-performance scaling**.

---

## ğŸ› ï¸ **Aurora Storage System â€“ Self-Healing, Fault-Tolerant**

### ğŸ”¹ **10 GB Protection Groups**

- Data is split into **units of 10 GB**, called Protection Groups.
- Each group is **replicated across 3 AZs**, with **6 total copies**.

### ğŸ”¹ **Write Durability via Quorum**

- A write is successful if **4 of 6 nodes** acknowledge it.
- Even if **2 nodes fail**, the system continues without issue.

### ğŸ”¹ **Self-Healing**

- If any node fails:

  - Aurora **reconstructs the missing data** from surviving replicas.
  - **No impact** on database availability or performance.

---

## ğŸ‘©ğŸ»â€âš–ï¸ **Aurora vs. Traditional RDS â€“ Key Differences**

| Feature               | **Aurora**                                      | **Amazon RDS**                                 |
| --------------------- | ----------------------------------------------- | ---------------------------------------------- |
| **Storage Model**     | **Shared, distributed storage (multi-AZ)**      | **EBS-based storage (single AZ per instance)** |
| **Replication**       | **Storage-level replication (no WAL shipping)** | **Streaming WAL logs to replicas**             |
| **Replication Lag**   | **Milliseconds (<100ms)**                       | **Seconds to minutes**                         |
| **Failover Time**     | **~30 seconds**                                 | **60-120 seconds**                             |
| **Read Replicas**     | **Up to 15, zero-lag**                          | **Up to 5, replication lag exists**            |
| **Storage Scaling**   | **Auto-scales to 128TiB**                       | **Manual resizing required**                   |
| **Backup & Recovery** | **Continuous, no performance impact**           | **Daily snapshots (affects performance)**      |

ğŸ“Œ **Key Takeaways:**

- âœ” **Aurora eliminates replication lag by sharing a single storage layer.**
- âœ” **Aurora failover is ~30s, RDS failover takes ~1-2 minutes.**
- âœ” **Aurora scales automatically, RDS requires manual resizing.**

---

## ğŸ† **6ï¸âƒ£ Unique Aurora Features**

### ğŸŒ **Aurora Global Database**

- ğŸ” Replicates across **multiple AWS regions**.
- ğŸŒ Cross-region lag **<1s**.
- ğŸ›¡ Great for disaster recovery and global reads.

### âš™ï¸ **Aurora Serverless (v2)**

- Auto-scales **compute capacity**, not just storage.
- Perfect for **variable workloads and dev/test** environments.

### ğŸ§¬ **Aurora Cloning**

- âš¡ Instantly clone databases **without copying full data**.
- Use cases: **QA, testing, analytics**.

### ğŸ”„ **Cluster Cache Management**

- Promoted replicas **inherit warm cache**.
- Prevents performance drops after failover (unlike RDS).

---

## ğŸ¯ **Final Summary: Why Auroraâ€™s Architecture is Superior**

| **Feature**               | **Why Aurora is Better**                                         |
| ------------------------- | ---------------------------------------------------------------- |
| **Auto-Scaling Storage**  | **Grows from 10GB to 128TiB automatically**                      |
| **Shared Storage Layer**  | **Ensures zero-lag replication & fast failover**                 |
| **No WAL Shipping**       | **Writes are stored in shared storage, reducing network delays** |
| **Instant Read Replicas** | **Replicas do not require full data copies**                     |
| **Crash Recovery**        | **No need to replay logs, instant recovery**                     |

ğŸ’¡ **Final Thought**: **Auroraâ€™s architecture is designed for modern cloud scalability, offering better performance, durability, and cost efficiency than traditional RDS.** ğŸš€ğŸ”¥
