# ğŸ§ª Amazon Bedrock â€“ Fine-Tuning a Foundation Model

## ğŸ” What is Fine-Tuning in Bedrock?

> **Fine-tuning** is the process of adapting a **copy of a pre-trained Foundation Model (FM)** using **your own domain-specific data**, so the model becomes more aligned with your tasks or industry.

---

âœ… It **modifies the weights** of the model â€” making it _smarter in your context_.

---

<div style="text-align: center;">
    <img src="images/fine-tuning.png" style="border-radius: 10px; width: 40%;" alt="Fine-tuning">
</div>

---

## âš™ï¸ Bedrock Fine-Tuning Requirements

| Requirement               | Description                                                                    |
| ------------------------- | ------------------------------------------------------------------------------ |
| ğŸ“ Data Format            | Must follow Bedrockâ€™s **structured format** (JSONL, single-turn or multi-turn) |
| â˜ï¸ Data Storage           | Training data must be in **Amazon S3**                                         |
| ğŸ“ˆ Provisioned Throughput | Fine-tuned models require **reserved throughput** to run                       |
| âš ï¸ Model Support          | **Not all FMs** in Bedrock support fine-tuning                                 |

---

## ğŸ§  Two Types of Fine-Tuning

### 1. ğŸ“ Instruction-Based Fine-Tuning

> Teaches the model using **prompt-response** examples (supervised learning)

---

<div style="text-align: center;">
    <img src="images/instruction-based-fine-tuning.png" style="border-radius: 10px; width: 40%;" alt="Instruction-based fine-tuning">
</div>

---

| Feature     | Description                                                             |
| ----------- | ----------------------------------------------------------------------- |
| ğŸ“Œ Format   | Prompt â†’ Expected response (like Q&A or instruction-output)             |
| âœ… Benefit  | Best for **domain-specific tasks** and behaviors                        |
| ğŸ§ª Use Case | Train chatbot to respond like a customer agent, ad copy generator, etc. |

**Single-Turn Example**:

```json
{
  "system": "You are an AWS tutor.",
  "messages": [
    { "role": "user", "content": "Explain S3 versioning." },
    { "role": "assistant", "content": "S3 versioning allows..." }
  ]
}
```

**Multi-Turn Example**:

```json
{
  "messages": [
    { "role": "user", "content": "Hi, whatâ€™s SageMaker used for?" },
    { "role": "assistant", "content": "SageMaker helps you train ML models..." },
    { "role": "user", "content": "Can I use it for tuning?" },
    { "role": "assistant", "content": "Yes, you can use AMT..." }
  ]
}
```

---

### 2. ğŸ§  Continued Pre-training (Domain Adaptation)

> You feed **unlabeled data** to the model to make it familiar with specific terminology or knowledge.

---

<div style="text-align: center;">
  <img src="images/continued-pre-training-fine-tuning.png"
       style="border-radius: 10px; width: 40%;"
       alt="continued-pre-training-fine-tuning">
</div>

---

| Feature       | Description                                                          |
| ------------- | -------------------------------------------------------------------- |
| ğŸ“š Input Type | Raw text â€” no prompt/response structure needed                       |
| ğŸ’¼ Use Case   | Feed **company manuals**, **industry docs**, or **AWS docs**         |
| ğŸ” Dynamic    | Can keep updating as **more domain-specific data becomes available** |

---

## ğŸ’¡ Transfer Learning vs. Fine-Tuning

| Concept              | Meaning                                                             |
| -------------------- | ------------------------------------------------------------------- |
| ğŸ§  Transfer Learning | Reusing knowledge from one task/model to apply it to another task   |
| ğŸ§ª Fine-Tuning       | A **specific form of transfer learning** where you update the model |

ğŸ“Œ Used heavily in NLP models like BERT, GPT â€” also appears on exams!

---

## ğŸ’° Cost & Expertise Considerations

| Category              | Notes                                                              |
| --------------------- | ------------------------------------------------------------------ |
| ğŸ’¸ Training Cost      | **Expensive** for large models; continued pre-training is costlier |
| ğŸ’¡ Instruction Tuning | More efficient â€” less compute and fewer examples required          |
| ğŸ‘¨â€ğŸ”§ Skill Level Needed | Requires **ML engineers** to prep data, fine-tune, evaluate        |
| ğŸ’µ Runtime Cost       | Running fine-tuned models requires **provisioned throughput**      |

---

## ğŸ¯ Common Fine-Tuning Use Cases

| Use Case                     | Purpose                                                     |
| ---------------------------- | ----------------------------------------------------------- |
| ğŸ¤– Chatbot with tone/persona | Create AI with specific behavior or style                   |
| ğŸ“¬ Exclusive internal data   | Train on customer messages, helpdesk chats, internal docs   |
| ğŸ“… Updated knowledge         | Teach model recent data beyond its original training cutoff |
| ğŸ“Š Business classification   | Custom model for ticket tagging, sentiment scoring, etc.    |

---

## âœï¸ Smart Summary Table

| Area                      | Quick Notes                                                 |
| ------------------------- | ----------------------------------------------------------- |
| ğŸ”§ Instruction Tuning     | Labeled prompt/response data â†’ domain task optimization     |
| ğŸ“š Continued Pre-training | Unlabeled data â†’ improve model knowledge on specific domain |
| ğŸ§  Transfer Learning      | General concept; fine-tuning is one specific method         |
| ğŸ’° Cost Consideration     | Training + serving costs are higher, but value is immense   |
